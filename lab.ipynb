{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9949a8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn import neighbors\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f42d513",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = pd.read_csv(\"./files_for_lab/categorical.csv\")\n",
    "numerical = pd.read_csv(\"./files_for_lab/numerical.csv\")\n",
    "targets = pd.read_csv(\"./files_for_lab/target.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88301ea",
   "metadata": {},
   "source": [
    "# Upscaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fdcc76",
   "metadata": {},
   "source": [
    "### x,y split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc90b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([categorical,numerical],axis=1)\n",
    "y = targets['TARGET_B']\n",
    "yD = targets['TARGET_D']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f047c74",
   "metadata": {},
   "source": [
    "### train - test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fc5155",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "yD_train = yD[y_train.index]\n",
    "yD_test = yD[y_test.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc1bf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(X_train),len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8264155a",
   "metadata": {},
   "source": [
    "### Normalizing & onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1303350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is not necessary for the tree model but I want to make a pipeline to try different models\n",
    "# and some of those do require scaling. \n",
    "\n",
    "# NUM-CAT split\n",
    "X_train_num = X_train.select_dtypes(np.number)\n",
    "X_train_cat = X_train.select_dtypes(object)\n",
    "\n",
    "# Normalizing train\n",
    "transformer = MinMaxScaler().fit(X_train_num)\n",
    "X_train_num = transformer.transform(X_train_num)\n",
    "X_train_num = pd.DataFrame(X_train_num, columns=X_train.select_dtypes(np.number).columns)\n",
    "\n",
    "# Onehot train\n",
    "encoder = OneHotEncoder(handle_unknown='error',drop='first').fit(X_train_cat)\n",
    "encoded = encoder.transform(X_train_cat).toarray()\n",
    "onehot_encoded = pd.DataFrame(encoded,columns=encoder.get_feature_names_out(X_train_cat.columns))\n",
    "\n",
    "# Concatenating back to create the transformed X_train\n",
    "X_train = pd.concat([X_train_num,onehot_encoded],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9401ce4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformign X_test for later:\n",
    "\n",
    "# NUM-CAT split\n",
    "X_test_num = X_test.select_dtypes(np.number)\n",
    "X_test_cat = X_test.select_dtypes(object)\n",
    "\n",
    "# Normalizing train\n",
    "X_test_num = transformer.transform(X_test_num)\n",
    "X_test_num = pd.DataFrame(X_test_num, columns=X_test.select_dtypes(np.number).columns)\n",
    "\n",
    "# Onehot train\n",
    "encoded = encoder.transform(X_test_cat).toarray()\n",
    "onehot_encoded = pd.DataFrame(encoded,columns=encoder.get_feature_names_out(X_test_cat.columns))\n",
    "\n",
    "# Concatenating back to create the transformed X_train\n",
    "X_test = pd.concat([X_test_num,onehot_encoded],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624a8c46",
   "metadata": {},
   "source": [
    "### Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d32086",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595f01bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.reset_index(drop=True)\n",
    "trainset = pd.concat([X_train,y_train],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75b9080",
   "metadata": {},
   "outputs": [],
   "source": [
    "category_0 = trainset[trainset['TARGET_B'] == 0]\n",
    "category_1 =trainset[trainset['TARGET_B'] == 1]\n",
    "\n",
    "category_1_oversample = resample(category_1,\n",
    "replace=True,\n",
    "n_samples = len(category_0))\n",
    "\n",
    "train_upsampled = pd.concat([category_0, category_1_oversample], axis=0)\n",
    "X_train_upsampled = train_upsampled.drop('TARGET_B',axis=1)\n",
    "y_train_upsampled = train_upsampled['TARGET_B']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d1aae2",
   "metadata": {},
   "source": [
    "### 1. Apply the Random Forests algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b28121",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Instead of applying only the random forest, I apply the pipeline we saw in class\n",
    "# to check every model score (with cross validation)\n",
    "\n",
    "model1 = DecisionTreeClassifier(max_depth=3)\n",
    "model2 = LogisticRegression(solver='saga', multi_class='ovr', n_jobs = -1)\n",
    "model3 = neighbors.KNeighborsClassifier(n_neighbors=5, weights='uniform', n_jobs = -1)\n",
    "model4 = RandomForestClassifier(max_depth=5,\n",
    "                             min_samples_split=20,\n",
    "                             min_samples_leaf =20,\n",
    "                             max_samples=0.8,\n",
    "                             n_jobs = -1)\n",
    "\n",
    "#data must be scaled here (it is)\n",
    "model_pipeline = [model1, model2, model3, model4]\n",
    "model_names = ['Decision Tree Classifier', 'Logistic Regression', 'KNN', 'Random Forest']\n",
    "scores = {}\n",
    "\n",
    "for model, model_name in zip(model_pipeline, model_names):\n",
    "    mean_score = np.mean(cross_val_score(model, X_train_upsampled, y_train_upsampled, cv=5,scoring ='accuracy'))\n",
    "    scores[model_name] = mean_score\n",
    "print(scores)\n",
    "\n",
    "# We can use the result to choose the best performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e400b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see that we basically get the same results with each model. However KNN \n",
    "# seems strangely huge. We must have into account that this score is the trainning score.\n",
    "# Maybe the KNN is just overfitted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5215f4",
   "metadata": {},
   "source": [
    "## 2. Use Feature Selections that you have learned in class to decide if you want to use all of the features (PCA, etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b27842",
   "metadata": {},
   "source": [
    "### 2.1 Numericals\n",
    "I won't drop any categorical because we only have 7. \n",
    "\n",
    "Instead of using PCA, I'll check both the variance and chi2 of the numerical features with our target, and drop those who have low variance and low \"correlation\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f791a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical = X_train_num.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8efff57",
   "metadata": {},
   "source": [
    "#### Checking variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fd592c",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_threshold = 0.02\n",
    "sel = VarianceThreshold(threshold=(var_threshold))\n",
    "\n",
    "# This drops the columns that have a variance less than this threshold\n",
    "sel = sel.fit(numerical.select_dtypes(np.number))\n",
    "#temp = pd.DataFrame(sel.transform(X_train.select_dtypes(np.number)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f19e686",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel.get_support()\n",
    "var_list = list(sel.get_support())\n",
    "num_var = pd.DataFrame(np.c_[var_list,numerical.columns],columns = ['Var_check', 'Name'])\n",
    "num_var = num_var[['Name','Var_check']]\n",
    "num_var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a527b0",
   "metadata": {},
   "source": [
    "#### Checking  chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081a759b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To check the scores\n",
    "model = SelectKBest(chi2, k=10).fit(numerical, y_train)\n",
    "df = pd.DataFrame(data = model.scores_, columns = ['score'])\n",
    "df['Column'] = numerical.columns\n",
    "\n",
    "aux = df.sort_values(by='score',ascending = False).reset_index()\n",
    "\n",
    "# ranking by chi2_score\n",
    "for i in range(len(aux)):\n",
    "    aux.loc[i,'chi2_rank'] = i+1\n",
    "num_chi2 = aux.sort_values(by='index').set_index('index',drop=True)\n",
    "num_chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e2dfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_select_features = pd.merge(left = num_var, right = num_chi2, how = 'inner', left_on = 'Name', right_on = 'Column')[['Column','Var_check','chi2_rank']]\n",
    "num_select_features\n",
    "# (chi2_rank should be computed everytime I drop a column but that would take too much\n",
    "# time so I'll skip that since this lab is pretty long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a852863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I want to see which  have both a low chi2_rank and a low variance.\n",
    "\n",
    "# Instead of throwing everything with low variance, I'll throw those that are not\n",
    "# in the top 50 chi2_rank\n",
    "dropping = num_select_features[(num_select_features['Var_check']==False)&(num_select_features['chi2_rank']>50)]\n",
    "dropping = list(dropping['Column'])\n",
    "len(dropping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9a834d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_upsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fe3790",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in dropping:\n",
    "    X_train_upsampled = X_train_upsampled.drop([column],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c2a3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Instead of applying only the random forest, I apply the pipeline we saw in class\n",
    "# to check every model score (with cross validation)\n",
    "\n",
    "model1 = DecisionTreeClassifier(max_depth=3)\n",
    "model2 = LogisticRegression(solver='saga', multi_class='ovr', n_jobs = -1)\n",
    "model3 = neighbors.KNeighborsClassifier(n_neighbors=5, weights='uniform', n_jobs = -1)\n",
    "model4 = RandomForestClassifier(max_depth=5,\n",
    "                             min_samples_split=20,\n",
    "                             min_samples_leaf =20,\n",
    "                             max_samples=0.8,\n",
    "                             n_jobs = -1)\n",
    "\n",
    "#data must be scaled here (it is)\n",
    "model_pipeline = [model1, model2, model3, model4]\n",
    "model_names = ['Decision Tree Classifier', 'Logistic Regression', 'KNN', 'Random Forest']\n",
    "scores = {}\n",
    "\n",
    "for model, model_name in zip(model_pipeline, model_names):\n",
    "    mean_score = np.mean(cross_val_score(model, X_train_upsampled, y_train_upsampled, cv=5,scoring ='accuracy'))\n",
    "    scores[model_name] = mean_score\n",
    "print(scores)\n",
    "\n",
    "# We can use the result to choose the best performing model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada49688",
   "metadata": {},
   "source": [
    "## 3. Discuss the output and its impact in the bussiness scenario. Is the cost of a false positive equals to the cost of the false negative? How would you change your algorithm or data in order to maximize the return of the bussiness?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccd135d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A false positive is not a big deal, becuase contacting by mail is cheap, but a false\n",
    "# negative shoud be avoided because that's a real donation that you'll be missing (and\n",
    "# the mean money donated by donators is way bigger than one mailing cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a80f812",
   "metadata": {},
   "source": [
    "# Lab | Final regression model in \"Health Care for All\" Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ce39491",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = pd.read_csv(\"./files_for_lab/categorical.csv\")\n",
    "numerical = pd.read_csv(\"./files_for_lab/numerical.csv\")\n",
    "targets = pd.read_csv(\"./files_for_lab/target.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4accf897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating \n",
    "data = pd.concat([categorical,numerical,targets['TARGET_D']],axis=1)\n",
    "# Filtering out non donors\n",
    "data = data[data['TARGET_D']!=0].reset_index(drop=True)\n",
    "\n",
    "X = data.drop(['TARGET_D'],axis=1)\n",
    "y = data['TARGET_D']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d04749df",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d3b5d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUM-CAT split\n",
    "X_train_num = X_train.select_dtypes(np.number)\n",
    "X_train_cat = X_train.select_dtypes(object)\n",
    "\n",
    "# Normalizing train\n",
    "transformer = MinMaxScaler().fit(X_train_num)\n",
    "X_train_num = transformer.transform(X_train_num)\n",
    "X_train_num = pd.DataFrame(X_train_num, columns=X_train.select_dtypes(np.number).columns)\n",
    "\n",
    "# Onehot train\n",
    "encoder = OneHotEncoder(handle_unknown='error',drop='first').fit(X_train_cat)\n",
    "encoded = encoder.transform(X_train_cat).toarray()\n",
    "onehot_encoded = pd.DataFrame(encoded,columns=encoder.get_feature_names_out(X_train_cat.columns))\n",
    "\n",
    "# Concatenating back to create the transformed X_train\n",
    "X_train = pd.concat([X_train_num,onehot_encoded],axis=1)\n",
    "y_train = y_train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a10aa969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Decision Tree Regressor': 0.03373327772351644, 'Linear Regression': 0.25838096309728054, 'KNN': 0.12278734103063105, 'Random Forest': 0.501674755112925}\n"
     ]
    }
   ],
   "source": [
    "model1 = DecisionTreeRegressor(max_depth=10)\n",
    "model2 = LinearRegression()\n",
    "model3 = KNeighborsRegressor(n_neighbors=5)\n",
    "model4 = RandomForestRegressor(max_depth=10,\n",
    "                             min_samples_split=20,\n",
    "                             min_samples_leaf =20,\n",
    "                             max_samples=0.8,\n",
    "                             n_jobs = -1)\n",
    "\n",
    "#data must be scaled here\n",
    "model_pipeline = [model1, model2, model3, model4]\n",
    "model_names = ['Decision Tree Regressor', 'Linear Regression', 'KNN','Random Forest']\n",
    "\n",
    "scores = {}\n",
    "for model, model_name in zip(model_pipeline, model_names):\n",
    "    mean_score = np.mean(cross_val_score(model, X_train, y_train, cv=5,scoring='r2'))\n",
    "    scores[model_name] = mean_score\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cc481f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5472829696857815"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For this case it really seems like a simple linear regression give back better results\n",
    "from sklearn import linear_model\n",
    "lm = linear_model.LinearRegression()\n",
    "lm.fit(X_train,y_train)\n",
    "lm.score(X_train,y_train)\n",
    "\n",
    "# but the result is still awful, I'm not sure if I did something wrong..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
